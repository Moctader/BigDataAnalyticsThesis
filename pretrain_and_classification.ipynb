{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Section\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Concatenate, Dropout, BatchNormalization, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Section\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Path to the folder with all data\n",
    "PREFIX = \"/Users/moctader/Thesis_code\"\n",
    "\n",
    "# Zoom level\n",
    "ZOOM_LEVEL = 10\n",
    "\n",
    "\n",
    "csv_path = f\"{PREFIX}/GTK_ASsoil_obs.csv\"\n",
    "base_directory = f\"{PREFIX}/output20/\"\n",
    "\n",
    "\n",
    "def read_geo_data(csv_path):\n",
    "    points = gpd.read_file(csv_path)\n",
    "    return points\n",
    "points=read_geo_data(csv_path)\n",
    "\n",
    "\n",
    "# Data points\n",
    "points.POINT_X = points.POINT_X.astype(\"float\")\n",
    "points.POINT_Y = points.POINT_Y.astype(\"float\")\n",
    "\n",
    "#samples\n",
    "\n",
    "samples = gpd.GeoDataFrame(\n",
    "    points.CLASS, crs=\"EPSG:3067\", geometry=gpd.points_from_xy(points.POINT_X, points.POINT_Y)\n",
    ").to_crs(\"WGS84\")\n",
    "\n",
    "tile_list = [(point.x, point.y) for point in samples['geometry']]\n",
    "\n",
    "\n",
    "# Creating image filename\n",
    "\n",
    "samples[\"i\"] = samples.index\n",
    "samples[\"filenames\"] = samples.apply(lambda row: f\"{row['CLASS']}/image_{row['i']}\", axis=1)\n",
    "\n",
    "\n",
    "# Extracting Latitude and Longitude from GeoDataFrame\n",
    "\n",
    "def get_lat_from_row(p):\n",
    "    lon, lat = p.geometry.x, p.geometry.y\n",
    "    return lat\n",
    "\n",
    "def get_lon_from_one_column(geometry):\n",
    "    lon, lat = geometry.x, geometry.y\n",
    "    return lon\n",
    "\n",
    "# Calculating Latitude for Each Sample\n",
    "\n",
    "samples[\"lat\"] = samples.apply(lambda row: get_lat_from_row(row), axis=1)\n",
    "\n",
    "# Calculating Longitude for Each Sample\n",
    "\n",
    "samples[\"lon\"] = samples[\"geometry\"].map(get_lon_from_one_column)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "vgg16_model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m feature_layer \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mblock1_pool\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m feature_extractor \u001b[39m=\u001b[39m Model(inputs\u001b[39m=\u001b[39mvgg16_model\u001b[39m.\u001b[39minput, outputs\u001b[39m=\u001b[39mvgg16_model\u001b[39m.\u001b[39mget_layer(feature_layer)\u001b[39m.\u001b[39moutput)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Features and Labels\u001b[39;00m\n\u001b[1;32m      5\u001b[0m features \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_layer = 'block1_pool'\n",
    "feature_extractor = Model(inputs=vgg16_model.input, outputs=vgg16_model.get_layer(feature_layer).output)\n",
    "\n",
    "# Features and Labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Loop through all images\n",
    "def load_data(filename, directory):\n",
    "    img_path = directory + \"/\" + filename + \".png\"\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "    # Extract features using the VGG16 model\n",
    "    vgg16_features = feature_extractor.predict(img_array)\n",
    "\n",
    "    # Flatten the VGG16 features to a 1D array\n",
    "    flattened_vgg16_features = vgg16_features.flatten()\n",
    "\n",
    "    # Combine VGG16 features with existing features (central pixel values)\n",
    "\n",
    "    features.append(flattened_vgg16_features)\n",
    "\n",
    "    # Assign labels based on the path\n",
    "    if \"/Ass/\" in img_path:\n",
    "        labels.append(1)\n",
    "    elif \"/Non-Ass/\" in img_path:\n",
    "        labels.append(0)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m file_names \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(base_directory)\n\u001b[1;32m      3\u001b[0m \u001b[39m# Filter out non-directory file_names\u001b[39;00m\n\u001b[1;32m      4\u001b[0m files \u001b[39m=\u001b[39m [file \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m file_names \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(base_directory, file))]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "file_names = os.listdir(base_directory)\n",
    "\n",
    "# Filter out non-directory file_names\n",
    "files = [file for file in file_names if os.path.isdir(os.path.join(base_directory, file))]\n",
    "\n",
    "for single_file in files:\n",
    "    samples[single_file] = samples[\"filenames\"].map(\n",
    "        lambda name, directory=os.path.join(base_directory, single_file): load_data(name, directory)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = len(vgg16_model.layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: input_1, Type: InputLayer, Output Shape: [(None, None, None, 3)]\n",
      "Layer 2: block1_conv1, Type: Conv2D, Output Shape: (None, None, None, 64)\n",
      "Layer 3: block1_conv2, Type: Conv2D, Output Shape: (None, None, None, 64)\n",
      "Layer 4: block1_pool, Type: MaxPooling2D, Output Shape: (None, None, None, 64)\n",
      "Layer 5: block2_conv1, Type: Conv2D, Output Shape: (None, None, None, 128)\n",
      "Layer 6: block2_conv2, Type: Conv2D, Output Shape: (None, None, None, 128)\n",
      "Layer 7: block2_pool, Type: MaxPooling2D, Output Shape: (None, None, None, 128)\n",
      "Layer 8: block3_conv1, Type: Conv2D, Output Shape: (None, None, None, 256)\n",
      "Layer 9: block3_conv2, Type: Conv2D, Output Shape: (None, None, None, 256)\n",
      "Layer 10: block3_conv3, Type: Conv2D, Output Shape: (None, None, None, 256)\n",
      "Layer 11: block3_pool, Type: MaxPooling2D, Output Shape: (None, None, None, 256)\n",
      "Layer 12: block4_conv1, Type: Conv2D, Output Shape: (None, None, None, 512)\n",
      "Layer 13: block4_conv2, Type: Conv2D, Output Shape: (None, None, None, 512)\n",
      "Layer 14: block4_conv3, Type: Conv2D, Output Shape: (None, None, None, 512)\n",
      "Layer 15: block4_pool, Type: MaxPooling2D, Output Shape: (None, None, None, 512)\n",
      "Layer 16: block5_conv1, Type: Conv2D, Output Shape: (None, None, None, 512)\n",
      "Layer 17: block5_conv2, Type: Conv2D, Output Shape: (None, None, None, 512)\n",
      "Layer 18: block5_conv3, Type: Conv2D, Output Shape: (None, None, None, 512)\n",
      "Layer 19: block5_pool, Type: MaxPooling2D, Output Shape: (None, None, None, 512)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Load pre-trained VGG16 model without the top (fully connected) layers\n",
    "\n",
    "# Iterate through the layers and print information\n",
    "for i, layer in enumerate(vgg16_model.layers):\n",
    "    print(f'Layer {i + 1}: {layer.name}, Type: {layer.__class__.__name__}, Output Shape: {layer.output_shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1192934036.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "vscode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7500c3e1c7c786e4ba1e4b4eb7588219b4e35d5153674f92eb3a82672b534f6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
