{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Section\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Concatenate, Dropout, BatchNormalization, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths \n",
    "PREFIX=\"/Users/moctader/Thesis_code/out/pickele\"\n",
    "\n",
    "Read_data=F\"{PREFIX}/samples.pkl\"\n",
    "\n",
    "# Read Data\n",
    "df=gpd.GeoDataFrame(\n",
    "    pd.read_pickle(Read_data),\n",
    "    geometry=\"geometry\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine channel and label extracted\n",
    "\n",
    "X = np.array([np.array(row['combined_channels']) for _, row in df.iterrows()])\n",
    "label = np.array(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the unique channels(arrays) form the combined channels\n",
    "unique_indices = [12, 10, 11, 20, 22, 23, 19,  4, 27, 24,  0,  9, 21,  3,  5, 15, 18, 6]\n",
    "features_5x5 = X[:, 23:28, 23:28, unique_indices]\n",
    "features_9x9 = X[:, 21:30, 21:30, unique_indices]\n",
    "features_15x15 = X[:, 18:33, 18:33, unique_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assaign other features lattitude and longitude \n",
    "\n",
    "latitude=np.array([np.array(row['lat']) for _, row in df.iterrows()])\n",
    "longitude=np.array([np.array(row['lon']) for _, row in df.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized the features\n",
    "features_5x5 = features_5x5/ 255.0\n",
    "features_9x9 = features_9x9/ 255.0\n",
    "features_15x15 = features_15x15/ 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the data size 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test split with the same number of samples\n",
    "X_feature_train, X_feature_test, X_scalar_train, X_scalar_test, y_train, y_test = train_test_split(\n",
    "    features_5x5,\n",
    "    np.column_stack((latitude, longitude)),\n",
    "    label,\n",
    "    test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model for processing image features\n",
    "input_feature = Input(shape=(5, 5, 18))\n",
    "\n",
    "# Convolutional layers with increasing filters, dropout, batch normalization\n",
    "x = Conv2D(8, (3, 3), activation='relu')(input_feature)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Define the input layer for scalar values\n",
    "input_scalar = Input(shape=(2,))  \n",
    "\n",
    "# Concatenate flattened features and scalar inputs\n",
    "merged_input = Concatenate()([x, input_scalar])\n",
    "\n",
    "# Hidden layer with fewer neurons, dropout, and batch normalization\n",
    "x = Dense(32, activation='relu')(merged_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_feature, input_scalar], outputs=output)\n",
    "\n",
    "# Use the Adam optimizer with a learning rate of 0.001\n",
    "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    [X_feature_train, X_scalar_train],\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    #callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate([X_feature_test, X_scalar_test], y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# Plot learning curve\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the data sixe 9x9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test split with the same number of samples\n",
    "X_feature_train, X_feature_test, X_scalar_train, X_scalar_test, y_train, y_test = train_test_split(\n",
    "    features_9x9,\n",
    "    np.column_stack((latitude, longitude)),\n",
    "    label,\n",
    "    test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model for processing image features\n",
    "input_feature = Input(shape=(9, 9, 18))\n",
    "\n",
    "# Convolutional layers with increasing filters, dropout, batch normalization, and max pooling\n",
    "x = Conv2D(8, (3, 3), activation='relu')(input_feature)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Define the input layer for scalar values\n",
    "input_scalar = Input(shape=(2,))  \n",
    "\n",
    "# Concatenate flattened features and scalar inputs\n",
    "merged_input = Concatenate()([x, input_scalar])\n",
    "\n",
    "# Additional hidden layer with fewer neurons, dropout, and batch normalization\n",
    "x = Dense(64, activation='relu')(merged_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_feature, input_scalar], outputs=output)\n",
    "\n",
    "# Use the Adam optimizer with a learning rate of 0.001\n",
    "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    [X_feature_train, X_scalar_train],\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    #callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate([X_feature_test, X_scalar_test], y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# Plot learning curve\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the data size 15x15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test split with the same number of samples\n",
    "X_feature_train, X_feature_test, X_scalar_train, X_scalar_test, y_train, y_test = train_test_split(\n",
    "    features_15x15,\n",
    "    np.column_stack((latitude, longitude)),\n",
    "    label,\n",
    "    test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model for processing image features\n",
    "input_feature = Input(shape=(15, 15, 18))\n",
    "\n",
    "# Convolutional layers with increasing filters, dropout, batch normalization, and max pooling\n",
    "x = Conv2D(8, (3, 3), activation='relu')(input_feature)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Define the input layer for scalar values\n",
    "input_scalar = Input(shape=(2,))  \n",
    "\n",
    "# Concatenate flattened features and scalar inputs\n",
    "merged_input = Concatenate()([x, input_scalar])\n",
    "\n",
    "# Additional hidden layer with fewer neurons, dropout, and batch normalization\n",
    "x = Dense(64, activation='relu')(merged_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_feature, input_scalar], outputs=output)\n",
    "\n",
    "# Use the Adam optimizer with a learning rate of 0.001\n",
    "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    [X_feature_train, X_scalar_train],\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    #callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate([X_feature_test, X_scalar_test], y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# Plot learning curve\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "vscode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7500c3e1c7c786e4ba1e4b4eb7588219b4e35d5153674f92eb3a82672b534f6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
